# 项目整体架构图





# 数据库和表结构

## 项目中是否进行了分库分表

使用了**Shardingsphere**中间件进行了**垂直分库**（按照不同的业务进行拆分）+ **水平分库**（每个业务内进行拆分）+ **水平分表**



## 为什么在用户表外额外设计用户手机表和用户邮箱表？

![密码登录.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1723691960114-1045b61f-03f3-455d-afa0-5e959558738f.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

用户登录时，可以用手机号或邮箱登录，也就是需要用手机号和邮箱来查询用户信息；而在订单业务中需要用用户id查询用户信息。分库分表使用的是用户id作为分片键，使用手机号 和 邮箱查询用户信息会造成**全路由问题**。



**解决方案：**

![用户登录.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723691907122-269cd4f3-d201-4759-aeeb-becaaad43b54.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

为了解决这个全路由问题，采用附属表方案，设置了用户手机表和用户邮箱表，分片键是手机号和邮箱，先通过手机号或邮箱查询到用户id，然后使用用户id查询用户表，这样就解决了问题。



**缺点：**

- 多了一步查询的过程，额外产生了性能的消耗
- 额外多了用户手机表和用户邮箱表，随着数据量的越来越大，表容量的占用也越来越大



## 项目中哪里使用了基因法

在订单业务中，即需要根据用户id查询订单，又需要根据订单号查询订单

假设分片数是N，用用户id的后$log_2N$位替换掉订单号的后$log_2N$位，二者对N的取模的结果相同。

- 可以保证一个用户所有订单号一定会分片同一个库和表中
- 通过该用户的id或订单号都可以直接定位到这张表上，不需要全路由



假设存在一种情况，在超高的并发下，在同一毫秒，同一台机器，生成两个id，那么这两个id唯一的区别就是序列号相差1，如果这时我们使用了基因法，那么这两个id不就重复了吗？

- 需要用户在同一毫秒下创建了2个订单才能重复，要是正常的用户肯定是不会重复的，除非是机器刷单或者恶意攻击这种情况



# 组件设计

## 如何实现高性能的Redisson延迟队列

![延迟队列.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723692675613-89c7d143-f893-408e-823a-d1e29f9d8461.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_65%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0)

**延迟消息发送者**：

消息发送者首先根据redissonClient和Topic。然后，构建消息发送处理器，获取**RBlockingQueue**和**RDelayedQueue**，每个Topic对应一个消息发送处理器，在消息发送处理器内部会维护一个List，这个List的大小为分区数，存储分区数个延迟队列，名字为`topic-i`，当有消息需要发送时，轮询从列表中选取一个延迟队列发送。



**延迟消息消费者**

首先定义一个任务接口**ConsumerTask**，内部包含进行消费需要执行的函数和这个任务对应的Topic。需要定义一个初始化器，在项目启动时初始化，获取所有的ConsumerTask，遍历ConsumerTask的Topic，对每个Topic建立分区数个延迟消费队列并启动消息监听，获取发送来的消息并调用ConsumerTask的进行消费函数。消息监听是通过while循环 + `RBlockingQueue.take()`阻塞等待任务实现的，并且是通过线程池创建出多线程进行异步监听和消费的，提高了消费的并发能力。



## 项目中的分布式锁是怎么设计的

分布锁是在经典的**Redisson**开源项目基础上，再次完善的封装，使用了**自定义注解 + Spring的AOP的方式，**另外也提供了方法级别



## 分布式锁与事务在生产中的“疑难杂症”

Spring中的事务本质上也是一个切面，这是如果在service方法加锁的话，这时也就是该方法上同时存在 **锁的切面** 和 **事务的切面**，Spring会将事务的切面和锁的切面放在一个切面 **有序集合** 中，然后依次的执行，这其实也是责任链模式。

<img src="C:/Users/shiyu/AppData/Roaming/Typora/typora-user-images/image-20250210212622974.png" alt="image-20250210212622974" style="zoom:50%;" />

问题就在开启事务和提交事务这部分，因为锁是在事务里面，所以 **开始事务和提交事务部分是没有被锁住的。**

既然知道了原因，那么解决办法就是将锁放到事务外，保证整个事务也被锁住即可解决

<img src="C:/Users/shiyu/AppData/Roaming/Typora/typora-user-images/image-20250210212759417.png" alt="image-20250210212759417" style="zoom:50%;" />

答案就是使用**@order**  注解，让锁的切面的顺序先于事务，那么@order的值设置为多少合适呢，事务的order值默认为 **Integer.MAX_VALUE**，考虑到后续可能还要用到切面功能，也需要在锁切面的里面，所以这里设置为-10

```java
@Aspect
@Order(-10)
public class ServiceLockAspect {
    //省略
}
```





# 网关服务



# 用户服务



# 节目服务

## 如何实现高效的主页节目列表显示

![主页列表.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720513872828-1b7678a2-0438-429a-880c-a81ae09ba6a4.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_54%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#selectHomeList**

```java
@ApiOperation(value = "查询主页列表")
@PostMapping(value = "/home/list")
public ApiResponse<List<ProgramHomeVo>> selectHomeList(@Valid @RequestBody ProgramListDto programPageListDto) {
    return ApiResponse.ok(programService.selectHomeList(programPageListDto));
}
```

**问题**：

- 可以看到，主页列表的查询条件是**区域id**和**父节目类型id**的集合，但节目表的分片键是**主键id**，会造成读扩散导致全路由查询，效率低。
- 且无法用基因法解决，**基因法的查询只能用一个条件**，不能两个条件同时查询。

**解决方法**：

- 可以借助分布式的查询引擎**elasticsearch**，将节目表的数据存放到elasticsearch中
- elasticsearch中只存放节目表的数据，余票数量还是放到redis中，这样就不能在余票数据不一致的问题了
- 至于内存压力的问题，可以设置定时任务，将过期的节目从elasticsearch中删除掉，elasticsearch只保存可以订票的节目



## 如何保证elasticsearch和数据库的数据一致性

**1. 应用层同步双写**

- 确保先更新数据库后更新elasticsearch
- 如果数据库更新成功而elasticsearch更新失败，可以通过事务回滚来保证一致性

**2. 消息队列异步同步双写**

- 数据库变更写入消息队列生产者
- elasticsearch监听消息队列消费者并写入数据

**3. 使用定时任务扫表更新**

- 适合批量处理，可以在系统负载较低的时段运行
- 实时性差

**4. 使用Canal监听MySQL的binlog**

- 解析 `INSERT` / `UPDATE` / `DELETE` 操作
- 将变更同步到elasticsearch



## elasticsearch怎么进行初始化

在节目服务的数据初始化操作时，使用了**damai-service-initialize**初始化统一的组件，来实现对初始化操作的统一管理和执行顺序的管理

**四种服务启动后自动执行的方法**

- 实现CommanLineRunner接口

  ```java
  @Component
  public class TestCommandLineRunner implements CommandLineRunner {
      
      @Override
      public void run(final String... args) {
          System.out.println("======run执行======");
      }
  }
  ```

- 实现InitializingBean接口

  ```java
  @Component
  public class TestInitializingBean implements InitializingBean {
      
      @Override
      public void afterPropertiesSet() {
          System.out.println("======afterPropertiesSet执行======");
      }
  }
  ```

- 使用PostConstruct注解

  ```java
  @Component
  public class TestPostConstruct {
      
      @PostConstruct
      public void postConstruct(){
          System.out.println("======postConstruct执行======");
      }
  }
  ```

- 实现ApplicationListener接口

  ```java
  @Component
  public class TestEventListener implements ApplicationListener<ApplicationStartedEvent> {
      
      @Override
      public void onApplicationEvent(ApplicationStartedEvent event) {
          System.out.println("======ApplicationStartedEvent执行======");
      }
  }
  ```

执行顺序是`InitializingBean` > `postConstruct` > `ApplicationStartedEvent` > `CommandLineRunner`



**如何按顺序管理各种初始化方式**

- 定义
  - 定义一个**InitializeHandler**接口，里面包含初始化执行类型、执行顺序和执行逻辑三个方法，每个初始化类型定义一个抽象类，实现**InitializeHandler**接口并实现其中的type方法。
  - 用户自定义的初始化需要继承对应的抽象类并实现执行顺序和执行逻辑这两个方法。

- 执行
  - 构建一个执行的公共抽象层，里面有一个excecute方法：
    - 从spring中获取InitializeHandler类型的bean集合
    - 将集合的type方法和当前实现类的type()方法进行匹配
    - 将集合中匹配到的元素通过executeOrder()方法进行排序
    - 将排序后的集合进行循环执行executeInit方法
  - 每个初始化类型实现一个执行类，执行类会在项目启动后执行执行的公共抽象层里的execute方法，进行每个类型的初始化



## 如何在分库分表情况下分页显示节目列表

![分类列表.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514047579-a6e11c7f-dc9a-48b5-bcc4-d5d235432fa9.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#selectPage**

```java
@ApiOperation(value = "查询分页列表")
@PostMapping(value = "/page")
public ApiResponse<PageVo<ProgramListVo>> selectPage(@Valid @RequestBody ProgramPageListDto programPageListDto) {
    return ApiResponse.ok(programService.selectPage(programPageListDto));
}
```



先是处理时间范围查询参数，然后去elasticsearch查询，如果查询不到，最后再去数据库中查询。之所以用elasticsearch查询的原因是，因为查询的条件中没有节目主键id分片键，所以在数据库中查询会造成读扩散全路由的问题，这个问题在主页列表显示中同样存在，也是用elasticsearch来解决的，但如果elasticsearch查询不到，那就只能去数据库中查询了



## 如何实现节目智能搜索功能

![搜索.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514657810-1fe866cd-d4c5-4b03-95e4-78e9dd3b70f9.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_36%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

搜索功能需要对明星或者节目标题进行搜索，而且可以将输入的内容进行**分词搜索**，直接使用elasticsearch



**接口：ProgramController#search**

```java
@ApiOperation(value = "搜索")
@PostMapping(value = "/search")
public ApiResponse<PageVo<ProgramListVo>> search(@Valid @RequestBody ProgramSearchDto programSearchDto) {
    return ApiResponse.ok(programService.search(programSearchDto));
}
```



**P.S. elasticsearch几种不同的查询方式**

- match 查询（全文搜索）

- term 查询（精确匹配）

- range 查询（范围搜索）

- bool 组合查询（must, should, filter 等）
  - `must`（必须匹配，影响得分）
  - `should`（可选匹配，提高得分）：匹配的文档得分更高
  - `filter`（必须匹配，不影响得分，性能最佳）

- aggregation（聚合查询）

- search_after（深度分页查询）



## 如何实现高性能节目详情展示功能

![节目详情.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514706282-02deed40-0600-43e2-8bf0-b08d50f06b43.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#getDetail**

```java
@ApiOperation(value = "查询详情(根据id)")
@PostMapping(value = "/detail")
public ApiResponse<ProgramVo> getDetail(@Valid @RequestBody ProgramGetDto programGetDto) {
    return ApiResponse.ok(programService.getDetail(programGetDto));
}
```

<img src="https://cdn.nlark.com/yuque/0/2024/png/22643320/1721723736110-45140271-c2fc-459c-8f9e-7457bc7022fb.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp" alt="查看节目详情.png" style="zoom:33%;" />

- 每个查询都是首先添加**分布式读锁**，写锁是在节目添加接口。使用读写锁可以减少锁的竞争。当其他用户进行查看节目详情时，都是读锁，读锁和读锁之间不会引起锁的竞争。
- 然后，先从redis中查询，如果redis中存在就直接返回
- redis中不存在的话，再从数据库中查询，再放入redis中



**为什么在这里预先加载用户购票人列表和用户节目订单数量**

- 用户购票人列表

  - 查询购票人信息的操作是在生成订单过程中，也就是**抢票业务**。抢票业务可是大麦项目的核心高并发业务，当大量的用户来抢票的话，这个购票人列表的查询可是对数据库的压力很大的，不夸张的说，当请求到达了一定量级了，数据库超时现象很快就能爆出来。所以需要使用缓存

  - **为什么不能在生成订单时把购票人信息放入缓存？**从业务入手，大麦网的高并发特点是用户会一瞬间抢购同一场演唱会，抢不到就抢不到了。下一次演唱会的抢购就不知道什么时候了。也就说**用户的购票人信息短时间内是不会复用的**，之所以放入缓存，是为了解决这一瞬间数据库的压力。也就是说放入缓存的时机要在生成订单前
  - **需要将所有用户的购票人信息放进缓存中吗**，如果都这么放入了，那缓存的内存压力会非常的大。登录的用户也不一定是要抢票的，但是**不登录的用户一定不是抢票的**，如果用户没登录，就不查询并放入缓存。

- 用户节目订单数量



**如何保证缓存和数据库中用户购票人的数据一致性**

- 至于缓存和数据库不一致的问题，很好解决，从业务上考虑，每个用户只会看自己的购票人信息。每个用户也只会修改自己的购票人信息。也就是说不会有修改购票人信息的并发问题。除非这个用户在不同的平台，比如app和pc同时的修改自己用户的购票人信息。不过谁也不会这么做。

- 当修改购票人信息后，直接将缓存中数据删除掉即可。等用户去看热门的演唱会时，还会将数据加载到缓存中



**其实查询节目详情这个接口还有进一步优化的空间**

- 使用CompletableFuture<T>异步加载节目详情中不同的部分
  - 使用 `CompletableFuture.supplyAsync()`异步执行每个查询
  - 使用`CompletableFuture.allOf(...).get();`登海所有查询完成



## 如何缓解缓存雪崩的

在缓存的过期时间设计上也进行了优化，将之前统一的设置过期时间，优化成了**根据所属节目的演出时间来设置**。

这样可以防止因设置统一过期时间而带来的同一时间产生的大量缓存过期，而带来的 **缓存雪崩** 问题



## 如何应对突发性热点数据暴增导致系统压力过大问题？

使用**缓存 + 双重检测锁**，保证只有第一个请求会去访问数据库，后面的请求直接走缓存，不走数据库

**双重检测锁的流程**

- 先从缓存中查询数据
- 如果缓存中不存在，则上锁查询库
- 分布式锁加锁
- 再从缓存中查询数据
- 缓存中还不存在，则从数据库中查询
- 将数据库中查询到的数据放到缓存中
- 分布式锁解锁
- 返回数据

但现在除了第一个请求外，从redis中查询是串行的，因为还需要竞争锁，效率低，可以进一步优化：

- 第一个请求获得了锁，从缓存中不存在，然后从数据库中查询再放入到缓存中
- 其余的请求等待1s来获得锁，如果超过1s，就不再竞争锁，程序继续往下执行，由于第一个请求将已经放数据放入到缓存中了，所以可以直接从缓存中查询到，将数据返回

<img src="https://cdn.nlark.com/yuque/0/2024/png/22643320/1712644645064-cfa44494-ce92-4eb6-a58c-88d61d545fe1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_28%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp" alt="Redis缓存+双重检测+tryLock.png" style="zoom:50%;" />

那么用这种方式是不是就不会有问题了呢？，答案 并不是。万一第一个请求的执行时间大于了1s，还没有来的及将数据库的数据放到缓存中，而这时其他的请求等待锁超过了1s，就会继续接着执行，仍然有可能集中到了数据库部分，数据库的压力还是会很大

这就需要根据业务来设置等待锁的时间，设置一个比较底限的值，保证不会超出的一个时间，但想设置一个百分之百不能超过的一个值，确实需要经过大量时间的业务运算来进行估算



# 订单服务

## 订单超时取消功能为什么使用Redisson而不是用RocketMQ

**RocketMQ的缺点**

- RocketMQ的延迟消息依赖固定的时间级别，例如 **1s、5s、10s、30s...**，但不支持自定义毫秒级的精确延迟。

- MQ **是基于 FIFO（先进先出）模式**，如果MQ发生消息堆积，RocketMQ的延迟消息可能会之后，导致任务无法按预期时间执行。

**Redisson的解决方案**

- Redisson **基于 Redis 的 `ZSet`（有序集合）** 进行延迟任务调度，任务按照 `score` 排序，Redisson 只会获取 **到期的任务**，不受未到期任务数量影响，**即使有 1000 万条任务堆积，Redisson 也只会取出到期任务进行消费**，而不会遍历所有任务。**可以做到毫秒级精度**，保证任务按时执行。
- **不会因消息堆积而影响执行时间**，即使 Redis 任务积压，也能通过**定时轮询** 精准执行任务。
- 当 Redisson 客户端重启时，`RDelayedQueue` 的状态会被自动恢复，因为其状态是持久化在 Redis 中的。这意味着即使应用重启，延迟队列的功能也不会受到影响



且能减少一个中间件依赖，因为几乎所有项目都会依赖redis





# 支付服务



# 其它问题

## 项目中遇到的最大的问题

查询节目详情需要查很多表，比如节目演出时间、节目数据、节目分组、用户节目订单数量、节目类型、节目票档等，是同步的尽管可以用redis和本地缓存查询，但可以使用CompletableFuture进行优化。



