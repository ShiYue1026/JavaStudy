# 项目整体架构图



# 数据库和表结构

## 项目中是否进行了分库分表

使用了**Shardingsphere**中间件进行了**垂直分库**（按照不同的业务进行拆分）+ **水平分库**（每个业务内进行拆分）+ **水平分表**



## 为什么在用户表外额外设计用户手机表和用户邮箱表？

![密码登录.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1723691960114-1045b61f-03f3-455d-afa0-5e959558738f.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

用户登录时，可以用手机号或邮箱登录，也就是需要用手机号和邮箱来查询用户信息；而在订单业务中需要用用户id查询用户信息。分库分表使用的是用户id作为分片键，使用手机号 和 邮箱查询用户信息会造成**全路由问题**。



**解决方案：**

![用户登录.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723691907122-269cd4f3-d201-4759-aeeb-becaaad43b54.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

为了解决这个全路由问题，采用附属表方案，设置了用户手机表和用户邮箱表，分片键是手机号和邮箱，先通过手机号或邮箱查询到用户id，然后使用用户id查询用户表，这样就解决了问题。



**缺点：**

- 多了一步查询的过程，额外产生了性能的消耗
- 额外多了用户手机表和用户邮箱表，随着数据量的越来越大，表容量的占用也越来越大



## 项目中哪里使用了基因法

在订单业务中，即需要根据用户id查询订单，又需要根据订单号查询订单

假设分片数是N，用用户id的后$log_2N$位替换掉订单号的后$log_2N$位，二者对N的取模的结果相同。

- 可以保证一个用户所有订单号一定会分片同一个库和表中
- 通过该用户的id或订单号都可以直接定位到这张表上，不需要全路由



假设存在一种情况，在超高的并发下，在同一毫秒，同一台机器，生成两个id，那么这两个id唯一的区别就是序列号相差1，如果这时我们使用了基因法，那么这两个id不就重复了吗？

- 需要用户在同一毫秒下创建了2个订单才能重复，要是正常的用户肯定是不会重复的，除非是机器刷单或者恶意攻击这种情况。
- 而且对单个用户的下单操作使用分布式锁的方式保证了幂等，不可能出现同一毫秒下同一个用户创建了2个订单。



# 组件设计

## 如何实现高性能的Redisson延迟队列

![延迟队列.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723692675613-89c7d143-f893-408e-823a-d1e29f9d8461.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_65%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0)

**延迟消息发送者**：

消息发送者首先根据redissonClient和Topic。然后，构建消息发送处理器，获取**RBlockingQueue**和**RDelayedQueue**，每个Topic对应一个消息发送处理器，在消息发送处理器内部会维护一个List，这个List的大小为分区数，存储分区数个延迟队列，名字为`topic-i`，当有消息需要发送时，轮询从列表中选取一个延迟队列发送。



**延迟消息消费者**

首先定义一个任务接口**ConsumerTask**，内部包含进行消费需要执行的函数和这个任务对应的Topic。需要定义一个初始化器，在项目启动时初始化，获取所有的ConsumerTask，遍历ConsumerTask的Topic，对每个Topic建立分区数个延迟消费队列并启动消息监听，获取发送来的消息并调用ConsumerTask的进行消费函数。消息监听是通过while循环 + `RBlockingQueue.take()`阻塞等待任务实现的，并且是通过线程池创建出多线程进行异步监听和消费的，提高了消费的并发能力。



## 项目中的分布式锁是怎么设计的

分布锁是在经典的**Redisson**开源项目基础上，再次完善的封装，使用了**自定义注解 + Spring的AOP的方式，**另外也提供了方法级别

![image-20250211191130150](https://github.com/user-attachments/assets/c329340d-599b-4a74-ac36-4de5b97d9deb)


大部分是在方法上加`@ServiceLock`注解，指定需要的锁类型、锁的名字和id，通过AOP的方式进行加锁，Spring AOP机制会使用`@Around`环绕拦截所有加了这个注解的方法，在其中通过SPEL机制获取锁的名字后进行加锁，加锁成功后通过切入点joinPoint执行业务逻辑，执行完成后释放锁。



## 分布式锁与事务在生产中的“疑难杂症”

Spring中的事务本质上也是一个切面，这是如果在service方法加锁的话，这时也就是该方法上同时存在 **锁的切面** 和 **事务的切面**，Spring会将事务的切面和锁的切面放在一个切面 **有序集合** 中，然后依次的执行，这其实也是责任链模式。

![image-20250210212622974](https://github.com/user-attachments/assets/ded0449c-d647-406d-bddb-fcad10f147f4)


问题就在开启事务和提交事务这部分，因为锁是在事务里面，所以 **开始事务和提交事务部分是没有被锁住的。**

既然知道了原因，那么解决办法就是将锁放到事务外，保证整个事务也被锁住即可解决

![image-20250210212759417](https://github.com/user-attachments/assets/aa69260c-7a5a-40ef-a61a-c4a85943590c)


答案就是使用**@order**  注解，让锁的切面的顺序先于事务，那么@order的值设置为多少合适呢，事务的order值默认为 **Integer.MAX_VALUE**，考虑到后续可能还要用到切面功能，也需要在锁切面的里面，所以这里设置为-10

```java
@Aspect
@Order(-10)
public class ServiceLockAspect {
    //省略
}
```



## 利用组合模式打造强大验证功能，轻松应对复杂验证需求

**问题**：使用`javax.validation`只能实现基础的规范性验证，但是对于具体的业务验证来说，还是需要我们自己来开发的，当接口变多了后，有些验证的逻辑是多个接口公共需要的，并且验证之间有顺序关系。



## 如何打造高效幂等组件，确保数据一致性

实现幂等的常见方法：

**1.唯一标识符**

- 为每个请求生成一个唯一的标识符（如UUID），并将其作为请求的一部分发送。当接收到请求时，服务器可以检查该标识符是否已处理过。如果已处理，则拒绝或忽略该请求；如果未处理，则处理该请求并记录标识符 

**2. 数据库唯一约束**

- 使用数据库的唯一约束（如主键或唯一索引）来确保即使多次尝试插入相同的数据，也只有一条记录会被保存。如果尝试插入重复的数据，数据库会抛出异常，服务器可以捕获这个异常并返回幂等性的响应 

**3. 乐观锁**

- 使用版本号或时间戳来检查数据是否已被其他操作修改过。在更新数据时，如果版本号或时间戳与预期的不符，则拒绝更新并返回冲突信息。这样，即使多次尝试更新相同的数据，也只有一次会成功

**4. 分布式锁**

- 在分布式系统中，可以使用分布式锁来确保同一时间只有一个节点能够执行某个操作。这可以防止多个节点同时处理相同的请求，从而实现幂等性 



而项目中的幂等性组件，不仅仅是单纯实现幂等，而是要在保证实现幂等的前提下，还要考虑高并发下的高效率执行，不能影响程序的性能和吞吐量



![幂等组件.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723692519364-f8d1eb6f-aeb7-405d-95bf-692f961a0ddb.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_24%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_619%2Climit_0)











# 网关服务



# 用户服务



# 节目服务

## 如何实现高效的节目主页显示

![主页列表.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720513872828-1b7678a2-0438-429a-880c-a81ae09ba6a4.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_54%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#selectHomeList**

```java
@ApiOperation(value = "查询主页列表")
@PostMapping(value = "/home/list")
public ApiResponse<List<ProgramHomeVo>> selectHomeList(@Valid @RequestBody ProgramListDto programPageListDto) {
    return ApiResponse.ok(programService.selectHomeList(programPageListDto));
}
```

**问题**：

- 可以看到，主页列表的查询条件是**区域id**和**父节目类型id**的集合，但节目表的分片键是**主键id**，会造成读扩散导致全路由查询，效率低。
- 且无法用基因法解决，**基因法的查询只能用一个条件**，不能两个条件同时查询。

**解决方法**：

- 可以借助分布式的查询引擎**elasticsearch**，将节目表的数据存放到elasticsearch中
- elasticsearch中只存放节目表的数据，余票数量还是放到redis中，这样就不存在余票数据不一致的问题了
- 至于内存压力的问题，可以设置定时任务，将过期的节目从elasticsearch中删除掉，elasticsearch只保存可以订票的节目



## 如何保证elasticsearch和数据库的数据一致性

**1. 应用层同步双写**

- 确保先更新数据库后更新elasticsearch
- 如果数据库更新成功而elasticsearch更新失败，可以通过事务回滚来保证一致性

**2. 消息队列异步同步双写**

- 数据库变更写入消息队列生产者
- elasticsearch监听消息队列消费者并写入数据

**3. 使用定时任务扫表更新**

- 适合批量处理，可以在系统负载较低的时段运行
- 实时性差

**4. 使用Canal监听MySQL的binlog**

- 解析 `INSERT` / `UPDATE` / `DELETE` 操作
- 将变更同步到elasticsearch



## elasticsearch怎么进行初始化

在节目服务的数据初始化操作时，使用了**damai-service-initialize**初始化统一的组件，来实现对初始化操作的统一管理和执行顺序的管理

**四种服务启动后自动执行的方法**

- 实现CommanLineRunner接口

  ```java
  @Component
  public class TestCommandLineRunner implements CommandLineRunner {
      
      @Override
      public void run(final String... args) {
          System.out.println("======run执行======");
      }
  }
  ```

- 实现InitializingBean接口

  ```java
  @Component
  public class TestInitializingBean implements InitializingBean {
      
      @Override
      public void afterPropertiesSet() {
          System.out.println("======afterPropertiesSet执行======");
      }
  }
  ```

- 使用PostConstruct注解

  ```java
  @Component
  public class TestPostConstruct {
      
      @PostConstruct
      public void postConstruct(){
          System.out.println("======postConstruct执行======");
      }
  }
  ```

- 实现ApplicationListener接口

  ```java
  @Component
  public class TestEventListener implements ApplicationListener<ApplicationStartedEvent> {
      
      @Override
      public void onApplicationEvent(ApplicationStartedEvent event) {
          System.out.println("======ApplicationStartedEvent执行======");
      }
  }
  ```

执行顺序是`InitializingBean` > `postConstruct` > `ApplicationStartedEvent` > `CommandLineRunner`



**如何按顺序管理各种初始化方式**

- 定义
  - 定义一个**InitializeHandler**接口，里面包含初始化执行类型、执行顺序和执行逻辑三个方法，每个初始化类型定义一个抽象类，实现**InitializeHandler**接口并实现其中的type方法。
  - 用户自定义的初始化需要继承对应的抽象类并实现执行顺序和执行逻辑这两个方法。

- 执行
  - 构建一个执行的公共抽象层，里面有一个excecute方法：
    - 从spring中获取InitializeHandler类型的bean集合
    - 将集合的type方法和当前实现类的type()方法进行匹配
    - 将集合中匹配到的元素通过executeOrder()方法进行排序
    - 将排序后的集合进行循环执行executeInit方法
  - 每个初始化类型实现一个执行类，执行类会在项目启动后执行执行的公共抽象层里的execute方法，进行每个类型的初始化



## 如何在分库分表情况下分页显示节目列表

![分类列表.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514047579-a6e11c7f-dc9a-48b5-bcc4-d5d235432fa9.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#selectPage**

```java
@ApiOperation(value = "查询分页列表")
@PostMapping(value = "/page")
public ApiResponse<PageVo<ProgramListVo>> selectPage(@Valid @RequestBody ProgramPageListDto programPageListDto) {
    return ApiResponse.ok(programService.selectPage(programPageListDto));
}
```



先是处理时间范围查询参数，然后去elasticsearch查询，如果查询不到，最后再去数据库中查询。之所以用elasticsearch查询的原因是，因为查询的条件中没有节目主键id分片键，所以在数据库中查询会造成读扩散全路由的问题，这个问题在主页列表显示中同样存在，也是用elasticsearch来解决的，但如果elasticsearch查询不到，那就只能去数据库中查询了



## 如何实现节目智能搜索功能

![搜索.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514657810-1fe866cd-d4c5-4b03-95e4-78e9dd3b70f9.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_36%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

搜索功能需要对明星或者节目标题进行搜索，而且可以将输入的内容进行**分词搜索**，直接使用elasticsearch



**接口：ProgramController#search**

```java
@ApiOperation(value = "搜索")
@PostMapping(value = "/search")
public ApiResponse<PageVo<ProgramListVo>> search(@Valid @RequestBody ProgramSearchDto programSearchDto) {
    return ApiResponse.ok(programService.search(programSearchDto));
}
```



**P.S. elasticsearch几种不同的查询方式**

- match 查询（全文搜索）

- term 查询（精确匹配）

- range 查询（范围搜索）

- bool 组合查询（must, should, filter 等）
  - `must`（必须匹配，影响得分）
  - `should`（可选匹配，提高得分）：匹配的文档得分更高
  - `filter`（必须匹配，不影响得分，性能最佳）

- aggregation（聚合查询）

- search_after（深度分页查询）



## 如何实现高性能节目详情展示功能

![节目详情.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1720514706282-02deed40-0600-43e2-8bf0-b08d50f06b43.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口：ProgramController#getDetail**

```java
@ApiOperation(value = "查询详情(根据id)")
@PostMapping(value = "/detail")
public ApiResponse<ProgramVo> getDetail(@Valid @RequestBody ProgramGetDto programGetDto) {
    return ApiResponse.ok(programService.getDetail(programGetDto));
}
```

<img src="https://cdn.nlark.com/yuque/0/2024/png/22643320/1721723736110-45140271-c2fc-459c-8f9e-7457bc7022fb.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_20%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp" alt="查看节目详情.png" style="zoom:33%;" />

- 每个查询都是首先添加**分布式读锁**，写锁是在节目添加接口。使用读写锁可以减少锁的竞争。当其他用户进行查看节目详情时，都是读锁，读锁和读锁之间不会引起锁的竞争。
- 然后，先从redis中查询，如果redis中存在就直接返回
- redis中不存在的话，再从数据库中查询，再放入redis中



**为什么在这里预先加载用户购票人列表和用户节目订单数量**

- 用户购票人列表

  - 查询购票人信息的操作是在生成订单过程中，也就是**抢票业务**。抢票业务可是大麦项目的核心高并发业务，当大量的用户来抢票的话，这个购票人列表的查询可是对数据库的压力很大的，不夸张的说，当请求到达了一定量级了，数据库超时现象很快就能爆出来。所以需要使用缓存

  - **为什么不能在生成订单时把购票人信息放入缓存？**从业务入手，大麦网的高并发特点是用户会一瞬间抢购同一场演唱会，抢不到就抢不到了。下一次演唱会的抢购就不知道什么时候了。也就说**用户的购票人信息短时间内是不会复用的**，之所以放入缓存，是为了解决这一瞬间数据库的压力。也就是说放入缓存的时机要在生成订单前
  - **需要将所有用户的购票人信息放进缓存中吗**，如果都这么放入了，那缓存的内存压力会非常的大。登录的用户也不一定是要抢票的，但是**不登录的用户一定不是抢票的**，如果用户没登录，就不查询并放入缓存。

- 用户节目订单数量



**如何保证缓存和数据库中用户购票人的数据一致性**

- 至于缓存和数据库不一致的问题，很好解决，从业务上考虑，每个用户只会看自己的购票人信息。每个用户也只会修改自己的购票人信息。也就是说不会有修改购票人信息的并发问题。除非这个用户在不同的平台，比如app和pc同时的修改自己用户的购票人信息。不过谁也不会这么做。

- 当修改购票人信息后，直接将缓存中数据删除掉即可。等用户去看热门的演唱会时，还会将数据加载到缓存中



**其实查询节目详情这个接口还有进一步优化的空间**

- 使用CompletableFuture<T>异步加载节目详情中不同的部分
  - 使用 `CompletableFuture.supplyAsync()`异步执行每个查询
  - 使用`CompletableFuture.allOf(...).get();`等待所有查询完成



## 如何保证查询节目时避免缓存穿透

缓存穿透在查询节目详情时同样存在，比如说某个黑客调用节目详情接口时，就会传入一个不存在的节目id，先查一遍缓存，缓存不存在则再去查询数据库，结果数据库也不存在，当并发高时，就会对数据库造成很大的压力



正常用户查询节目详情时，都是事先查询了节目主页或者节目列表后，再来查询节目详情的，也就是说节目id时肯定存在缓存或者数据库中的，如果是缓存和数据库都不存在的节目id说明此id就是不正常的，服务收到了此id就应该直接拒绝，不再执行后续的流程



注意这里的关键点，**如果判断节目id不存在，则说明此节目id肯定是不正常的**，这是不是和布隆过滤器的判断特点非常的贴合！**布隆过滤器的特点就是如果判断某个元素不存在，那么说明此元素一定就不存在！**



## 如何缓解缓存雪崩的

在缓存的过期时间设计上也进行了优化，将之前统一的设置过期时间，优化成了**根据所属节目的演出时间来设置**。

这样可以防止因设置统一过期时间而带来的同一时间产生的大量缓存过期，而带来的 **缓存雪崩** 问题



## 如何应对突发性热点数据暴增导致系统压力过大问题？

使用**缓存 + 双重检测锁**，保证只有第一个请求会去访问数据库，后面的请求直接走缓存，不走数据库

**双重检测锁的流程**

- 先从缓存中查询数据
- 如果缓存中不存在，则上锁查询库
- 分布式锁加锁
- 再从缓存中查询数据
- 缓存中还不存在，则从数据库中查询
- 将数据库中查询到的数据放到缓存中
- 分布式锁解锁
- 返回数据

但现在除了第一个请求外，从redis中查询是串行的，因为还需要竞争锁，效率低，可以进一步优化：

- 第一个请求获得了锁，从缓存中不存在，然后从数据库中查询再放入到缓存中
- 其余的请求等待1s来获得锁，如果超过1s，就不再竞争锁，程序继续往下执行，由于第一个请求将已经放数据放入到缓存中了，所以可以直接从缓存中查询到，将数据返回

<img src="https://cdn.nlark.com/yuque/0/2024/png/22643320/1712644645064-cfa44494-ce92-4eb6-a58c-88d61d545fe1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_28%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp" alt="Redis缓存+双重检测+tryLock.png" style="zoom:50%;" />

那么用这种方式是不是就不会有问题了呢？，答案 并不是。万一第一个请求的执行时间大于了1s，还没有来的及将数据库的数据放到缓存中，而这时其他的请求等待锁超过了1s，就会继续接着执行，仍然有可能集中到了数据库部分，数据库的压力还是会很大

这就需要根据业务来设置等待锁的时间，设置一个比较底限的值，保证不会超出的一个时间，但想设置一个百分之百不能超过的一个值，确实需要经过大量时间的业务运算来进行估算



## 如何保障节目数据在缓存与数据库之间的一致性

在用户抢票、支付及退单的过程中，余票数量的修改和座位状态的改变都是在缓存中操作的，并没有访问到数据库中的余票数量和座位操作。



**什么时候更新数据库中的余票数量和座位状态**

当支付回调后订单状态修改成了已支付、缓存中的余票扣除了、座位修改成已售卖了后，发送更新节目和座位的数据消息到延迟队列中，由节目服务来消费消息进行数据库中的更新



**数据库和缓存一致性问题**

在开始购票流程前，都要先去查看节目详情，也就是把数据都放到了缓存中。而在流程图中可以看到，在购票流程时，始终都是先用redis的余票数量来做验证，而在对余票数量进行扣减和还原时，也都是直接使用lua+redis中进行的修改，**所以并不会出现超卖的情况**



也就是说 **数据库和缓存的一致性带来的问题就进一步降低成了，只是显示不一致而已**



如果真出现不一致了，也就是数据库余票数量大于缓存中的余票数量，用户体验就会出现明明余票有数量但是购买时会出现余票不足的提示



但这个问题正好规避掉了，因为大麦网中并不显示余票数量



但话说回来，就算有余票数量显示的功能，数量不一致那又怎么样呢。本人在常见的订票系统像12306、携程、去哪网都订过票，发现它们的余票数量也都不是实时显示的，也会有订票了后，数量没有更新的时候，甚至过了10分钟后，数量也没更新



在大麦项目中，订单服务更新了缓存数据后，发送消息到延迟队列中，然后节目服务消费消息更新数据库中的数据，这里可以把延迟队列替换成Kafka或者其他的消息中间件，所以数据库和缓存的不一致的延迟时间取决于消息中间件的延迟时间，但消息中间件的效率执行起来非常的高，并不是想象中的那么脆弱。





## 用户选择座位的流程

![可选座说明.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1723690303276-99fd8e44-d367-45f4-9663-d78610fa23e9.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_13%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

![选择座位页面.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/22643320/1723690330801-1c896ebd-82da-43cb-9408-770402e85c27.jpeg?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_53%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0%2Finterlace%2C1)

**接口: SeatController#relateInfo**

```java
@ApiOperation(value = "查询座位相关信息")
@PostMapping(value = "/relate/info")
public ApiResponse<SeatRelateInfoVo> relateInfo(@Valid @RequestBody SeatListDto seatListDto) {
    return ApiResponse.ok(seatService.relateInfo(seatListDto));
}
```



1. 首先从redis中查询节目信息，如果查询不到，调用节目详情方法。**其实查询座位信息这步之前是要查询节目详情的，在查询节目详情中已经节目信息放到redis了中，这里再查询一遍也是为了保险，说白了，还是怕有意外将请求直接落到数据库上，造成数据库的压力过大**

2. 接下来，**查询座位信息、查询演出时间，然后将信息进行组装后返回给前端**

3. 通过遍历票档集合，根据节目id和票档id来查询出作为集合

4. 将查出来的座位按照 **未售卖、锁定中、已售卖** 状态进行分组后，分别存入redis中，这里使用Hash数据结构（<u>要把座位状态存到缓存中是因为，后续用户下单时涉及到座位状态的改变，为了高并发，这个过程肯定得在缓存中实现</u>）；此外，这里并不是一个节目的所有票都使用同一个键，而是<u>按照票档进行进一步的划分</u>，如果不同票档的数据在不同的redis实例上时就能避免抢不同票档的票之间的竞争。

   ![image-20250211193605183](C:/Users/shiyu/AppData/Roaming/Typora/typora-user-images/image-20250211193605183.png)



每次查询座位信息，都需要先从redis中查询，判断redis中是否存在，如果不存在要先去数据库中查询后再放入redis中。**这里有个问题就是因为分成了三个状态，需要分别查询三次，这几次查询之间就不是原子性的了，存在并发安全问题**。因此，需要使用**Lua脚本**保证这三次查询的原子性。



## 为什么座位信息不适合放在本地缓存中

座位信息是需要经常变更的，未售卖、已锁定和已售卖三种状态之间转换，本地缓存不能确保多个服务器节点之间的**数据一致性**，各个服务器之间无法感知到其它服务器上的座位信息变化，会导致超卖的情况。



## 如何统一管理复杂的用户购票验证流程

主要解决用户在进行购票流程时，如何将多个参数的业务验证逻辑进行统一管理起来，并且设置执行的先后顺序

用户购票时，需要验证：

- 验证作为参数
- 将节目缓存
- 验证缓存中是否存在节目数据
- 验证缓存是否存在演出时间数据
- 验证用户是否存在



![用户购票验证逻辑流程图.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1721726200696-262ce7db-8f52-4039-92ae-cd09c662c8fe.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_45%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



**接口：com.damai.service.ProgramOrderService#create**

```java
public String create(ProgramOrderCreateDto programOrderCreateDto) {
    //进行业务验证
    compositeContainer.execute(CompositeCheckType.PROGRAM_ORDER_CREATE_CHECK.getValue(),programOrderCreateDto);
    //省略...
}
```

**入参：com.damai.dto.ProgramOrderCreateDto**

```java
@Data
@ApiModel(value="ProgramOrderCreateDto", description ="节目订单创建")
public class ProgramOrderCreateDto {
    
    @ApiModelProperty(name ="programId", dataType ="Long", value ="节目id")
    @NotNull
    private Long programId;
    
    @ApiModelProperty(name ="userId", dataType ="Long", value ="用户id")
    @NotNull
    private Long userId;
    
    @ApiModelProperty(name ="ticketUserIdList", dataType ="List<Long>", value ="购票人id集合")
    @NotNull
    private List<Long> ticketUserIdList;
    
    @ApiModelProperty(name ="seatDtoList", dataType ="List<SeatDto>", value = "座位")
    private List<SeatDto> seatDtoList;
    
    @ApiModelProperty(name ="ticketCategoryId", dataType ="Long", value = "节目票档id(如果不选座位，那么票档id必填)")
    private Long ticketCategoryId;
    
    @ApiModelProperty(name ="ticketCount", dataType ="Integer", value = "购买票数量(如果不选座位，那么购买票数量必填)")
    private Integer ticketCount;
}
```

**1. ProgramOrderCreateParamCheckHandler**

- 验证入参中的购票人id是否有重复
- 验证入参中的座位信息是否正确
  - 手动选座：
    - 座位数量与购票人数量是否相等
    - 座位id不能为空
    - 节目票档id不能为空
    - 座位行号和列号不能为空
    - 座位价格不能为空
  - 自动选座：
    - 节目票档id不能为空
    - 购票数量不能为空
    - 购票数量不能负数



**2. ProgramDetailCheckHandler**

- 验证节目是否允许选座与入参中是否选座是否一致
- 验证购票数量是否超过限制



此验证逻辑是再次节目详情，放入到redis中。此流程其实在节目详情中已经执行过，这里**再次执行的原因的防止有人会绕过节目详情，直接调用购票接口，造成的数据库压力过大**，所以再次判断redis中是否存在，不存在的话放入redis中



**3. ProgramUserExistCheckHandler**

- 验证用户和购票人信息正确性
- 查询当前用户选择的节目下，已有该用户订单的数量，验证该用户该节目下的订单数量是否超过了限制



## 如何应对高并发下的购票压力

**接口：com.damai.controller.ProgramOrderController#createV1**

```java
@ApiOperation(value = "购票V1")
@PostMapping(value = "/create/v1")
public ApiResponse<String> createV1(@Valid @RequestBody ProgramOrderCreateDto programOrderCreateDto) {
    return ApiResponse.ok(programOrderLock.createV1(programOrderCreateDto));
}
```



**Service层**

```java
/**
 * 订单创建，使用节目id作为锁
 * */
@RepeatExecuteLimit(
        name = RepeatExecuteLimitConstants.CREATE_PROGRAM_ORDER,
        keys = {"#programOrderCreateDto.userId","#programOrderCreateDto.programId"})
@ServiceLock(name = PROGRAM_ORDER_CREATE_V1,keys = {"#programOrderCreateDto.programId"})
public String createV1(ProgramOrderCreateDto programOrderCreateDto) { compositeContainer.execute(CompositeCheckType.PROGRAM_ORDER_CREATE_CHECK.getValue(),programOrderCreateDto);
    return programOrderService.create(programOrderCreateDto);
}
```



![购票流程V1.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723690486895-141b34fe-21f2-451e-87eb-bc0055df99ca.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_69%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1125%2Climit_0)



**购票流程**：

- 对于每个票档，从缓存中查询全部座位信息，将未售卖的座位存到一个列表中，将每个票档余票数量存到一个Map中

- 获取入参中所需要购买的票的所有票档，检查入参中各个票档的购买数量是否小于该票档的余票数量
- 如果可以选座，遍历入参购买的每个座位，检查在未售卖座位列表中是否存在，并累加价格，最终检查购买价格和库中价格是否匹配
- 如果不能选座，利用算法自动根据人数和票档进行分配相邻座位，如果匹配出来的座位数量小于要购买的数量，拒绝执行
- 进行操作缓存中的数据，进行扣除余票数量和将对应的座位状态从未售卖修改为锁定中
  - 将传入的用户需要购买的座位列表按照票档ID进行分组，并统计每个分组的数量存入一个Map中
  - **更新余票数量**：遍历上面的Map，使用JSONObject记录需要在Redis中扣减的余票数量和Key值。
  - **未售卖 => 锁定中**：将传入的用户需要购买的座位列表按照票档ID进行分组后遍历每个票档中的座位信息，将删除座位的键、删除座位的id、添加座位的键、添加座位的id、添加座位的座位信息组装成JSONObject数组。
  - 将上面的JSONObject数组传入Lua脚本，原子性更新每个票档下的余票数量和座位信息
- 最后将筛选出来的需要购买的座位列表传入，执行创建订单的操作



## 如何对锁进行优化更好的缓解购票压力

**接口：com.damai.controller.ProgramOrderController#createV2**

```java
@ApiOperation(value = "购票V2")
@PostMapping(value = "/create/v2")
public ApiResponse<String> createV2(@Valid @RequestBody ProgramOrderCreateDto programOrderCreateDto) {
    return ApiResponse.ok(programOrderLock.createV2(programOrderCreateDto));
}
```



**锁的粒度**

![订单服务创建订单.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723690690916-36b332a9-ac73-4106-a92b-97e4169e7dca.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

在当前的用户购票方法中，分布式锁锁的是整个节目，锁的粒度过大，导致哪怕不同用户购买的是同一节目下的不同票档，也需要竞争同一把锁。而票的数量是按照票档进行划分的，不同票档完全不需要竞争同一把锁。



在同一时间内，这个节目只能有一个用户购票，这并发是比较差的，所以将锁的粒度进一步缩小，改成按照**节目id+座位票档id来加锁**，这样请求1、请求2、请求3、请求4就可以并发购票了，互不影响，系统的处理效率**直接提高400%**

![用户购票分布式锁V2.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723690743439-51e9c2b4-1355-40b5-930d-140258f4411a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

进行到这里确实将锁的效率提高了，但这样就没有问题了吗？问题确实存在，**如果请求1购买了一等票和二等票这两种类型的票，那么就有了锁重合的问题了**，虽然这种情况比较少，一般的人买多张都会买相同票档的，但确实可能会有这种情况出现，难道就没有办法了吗？并不是



**可以根据要购买的票档类型来获取多个锁**，还是以这个例子为例，这时请求1就要同时获取一等票和二等票两把锁了，执行完后再释放这两把锁

**如果有多把锁会不会有死锁的问题？**这里要把加锁的顺序和解锁的顺序处理好，以及还要把已获得锁的请求，即使在业务执行出现异常后也要能安全的解开，把这两点做好了，就不会出现死锁的问题了



**分布式锁的压力**

短时间大量请求竞争锁，对redis的压力不小，可以利用**本地锁**减少redis上的请求，先让在同一个实例中的请求去竞争自己实例的本地锁，当获得本地锁的请求再去竞争分布式锁，这样一来和之前的方案相比，短时间内的分布式锁的压力就会小很多了



**本地锁内存溢出问题**

个节目票档都会有一把本地锁，那么当请求多了，本地锁的对象也会增多，这就给系统造成内存压力，时间长了就会内存溢出。

所以要有一个过期时间的机制，当某个节目+票档在一段时间内没有访问的话，**那么就把对应的锁对象设置过期，等gc时直接回收掉。**



**为什么选择Caffeine**

Caffeine这样的本地缓存和ConcurrentMap很像，即支持并发，并且支持O(1)时间复杂度的数据存取。二者的主要区别在于：

- ConcurrentMap将存储所有存入的数据，直到你显式将其移除；
- Caffeine将通过给定的配置，自动移除“不常用”的数据，以保持内存的合理占用。

因此，一种更好的理解方式是：Cache是一种带有存储和移除策略的Map。



![用户购票分布式锁V2(加入本地锁).png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723690792208-2cd0aedb-e544-4b8b-8746-bcfb9c10c4c9.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



**加锁流程**

1. **初始化锁集合**：为每个票档ID创建对应的本地锁和分布式锁，本地锁收集到**localLockList**集合中，分布式锁收集到**serviceLockList**集合中
2. **加锁：** 

- **本地锁加锁**：遍历本地锁集合，尝试加锁，如果加锁成功，将该锁添加到加锁成功的本地锁**localLockSuccessList**集合中。如果加锁过程中遇到异常，则停止加锁流程。
- **分布式锁加锁**：遍历分布式锁集合，尝试加锁，如果加锁成功，将该锁添加到加锁成功的分布式锁**serviceLockSuccessList**集合中。同样，如果加锁过程中遇到异常，则停止加锁流程。

3. **执行业务逻辑**：在所有锁成功加锁之后，执行订单创建业务逻辑
4. **解锁：** 

- **分布式锁解锁**：从加锁成功的分布式锁集合中，按照逆序遍历解锁每一个分布式锁
- **本地锁解锁**：从加锁成功的本地锁集合中，按照逆序遍历解锁每一个本地锁



**本地锁 + 分布式锁的公平性问题**

就算本地锁和分布式锁都是公平锁了，用户获得锁的顺序就是公平的吗？

- 其实并不是

- 本地锁+分布式锁在全部采用公平锁的情况下，也只能保证局部有序，并不能保证全局有序

  ![本地锁+分布式锁公平性.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723690824820-4361ae1c-6f08-4e5f-a975-119afe7b3e84.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)






## 解决高并发下购票压力的终极杀招“无锁化”

**目前的抢票流程**

先获得锁，然后获取座位和余票信息，接着验证座位是否可以购买，余票是否充足。然后拼接lua需要的数据，接着在lua中完整扣减的操作

鸟巢应该算是容纳人数最多的演唱会场馆了，可以容纳9万人左右。而抢票的人有几百万很正常，**也就是说100个人也就一个人能抢到票**，能执行生成订单的流程，**那么有没有什么方案能在最开始的时候就把没有抢到票的人直接拒绝了呢？**



**目前存在的问题**

目前的逻辑确实有在座位逻辑那里，做了座位状态验证和余票数量验证逻辑，但它是从redis中获取然后再Java中做的验证，然后再组装数据在redis中运行修改状态，再加上开头都用了分布式锁也是借助redis，这多次操作redis，也就造成了多次的网络性能的消耗，要知道网络请求对性能的影响可以说是和慢sql的影响不相上下。

- 请求分布式锁
- 查询座位信息
- 查询余票数量
- 修改座位状态和余票数量



**既然开头用了分布式锁锁住了，那为何不全都在redis中执行呢**

一开始直接利用lua执行判断要购买的座位和票的数量是足够，不足够直接返回，足够的话进行相应扣除，这样既能将大量无用的抢购请求直接返回掉，又可以实现无锁化



虽然不需要分布式锁了，但幂等性的保证还是要依然实现的；且本地锁还是要保留的，因为本地锁的作用是为了避免无效的请求到redis上



座位状态和余票数量还是用过java和redis交互的，因为第一次启动redis中肯定是没有这些数据的，后面的全部放到lua脚本中执行

![bdc1ced9-4238-4fb0-a844-1e8101dc0415](C:/Users/shiyu/Desktop/bdc1ced9-4238-4fb0-a844-1e8101dc0415.png)



## 高并发场景下订单如何异步处理

V1版本：对节目id加分布式锁

V2版本：本地锁缓解redis请求压力 + 细化分布式锁粒度(节目id + 票档id)

V3版本：本地锁 + lua脚本实现无锁化

**V4版本：V3版本 + 异步下单**



当前V3版本使用本地锁 + lua脚本实现无锁化还存在什么瓶颈？

- 在调用订单服务生成订单这个步骤中，项目之间使用的是 **Feign** 的框架进行调用，Feign的底层其实是http调用，虽然可以替换连接池，比如 **feign-okhttp，**来提高性能，但是本质还是基于http的方式，性能就会受到影响
- 既然是高并发场景，在同一时间会有大量的订单请求，那么就要将生成订单的步骤变成异步，那么如何才能异步呢？这时就要借助mq中间件



**V4流程图**

![用户购票v4.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1720954650416-a9595510-d817-402b-a1e7-aacf2f9ea18f.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_50%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_817%2Climit_0)

1. 用户选择座位并下单后，向后端服务发送进行余票扣减的操作
2. redis通过V3中的lua脚本进行购票，购票失败直接通知用户购票失败
3. 执行完lua脚本成功购票后，把构建订单信息交给mq后，直接返回把订单编号返回给前端
4. 订单服务会一直消费mq的消息，收到后进行订单入库和订单入redis的操作
5. 虽然返回了订单编号，但由于异步场景，不一定真正的入库了（）下单成功），所以前端要有个定时任务，不断的去轮训订单是否真的存在，如果查询到了，才说明订单是真正的创建成功

6. 由于是高并发场景，会有大量的订单请求，所以前端的定时任务也不能一直的去轮训，要设置个超时时间，在项目中**设置的是在5s内，每隔200ms去轮训一次订单是否存在**，如果存在，则停止定时任务，订单生成成功，跳转支付页面，如果超过了5s还没有轮训到，那么直接通知用户生成订单失败

还有个问题，那就是前端轮训超过了5s没有查询到订单，提示了订单失败，但其实后端ma中间件消费延迟了，超过了5s才消费到，这时我们就不要再入库了，**直接把这个超过5s的订单丢弃！**



**构建订单发送给消息队列**

```java
private String doCreateV2(ProgramOrderCreateDto programOrderCreateDto,List<SeatVo> purchaseSeatList){
    //构建主订单和购票人订单信息
    OrderCreateDto orderCreateDto = buildCreateOrderParam(programOrderCreateDto, purchaseSeatList);
    //发送给kafka
    String orderNumber = createOrderByMq(orderCreateDto,purchaseSeatList);
    //延迟队列关闭订单发送
    DelayOrderCancelDto delayOrderCancelDto = new DelayOrderCancelDto();
    delayOrderCancelDto.setOrderNumber(orderCreateDto.getOrderNumber());
    delayOrderCancelSend.sendMessage(JSON.toJSONString(delayOrderCancelDto));
    
    return orderNumber;
}
```

```java
private String createOrderByMq(OrderCreateDto orderCreateDto,List<SeatVo> purchaseSeatList){
    CreateOrderMqDomain createOrderMqDomain = new CreateOrderMqDomain();
    CountDownLatch latch = new CountDownLatch(1);
    //发送kafka
    createOrderSend.sendMessage(JSON.toJSONString(orderCreateDto),sendResult -> {
        //发送成功
        createOrderMqDomain.orderNumber = String.valueOf(orderCreateDto.getOrderNumber());
        assert sendResult != null;
        log.info("创建订单kafka发送消息成功 topic : {}",sendResult.getRecordMetadata().topic());
        latch.countDown();
    },ex -> {
        //发送失败
        log.error("创建订单kafka发送消息失败 error",ex);
        log.error("创建订单失败 需人工处理 orderCreateDto : {}",JSON.toJSONString(orderCreateDto));
        updateProgramCacheDataResolution(orderCreateDto.getProgramId(),purchaseSeatList,OrderStatus.CANCEL);
        createOrderMqDomain.daMaiFrameException = new DaMaiFrameException(ex);
        latch.countDown();
    });
    try {
        //使用CountDownLatch等待发送结果
        latch.await();
    } catch (InterruptedException e) {
        log.error("createOrderByMq InterruptedException",e);
        throw new DaMaiFrameException(e);
    }
    //如果发送失败，则直接抛出异常
    if (Objects.nonNull(createOrderMqDomain.daMaiFrameException)) {
        throw createOrderMqDomain.daMaiFrameException;
    }
    return createOrderMqDomain.orderNumber;
}
```

在调用发送kafka的api时，使用了带有回调方式，当发送成功或者发送失败后，可以进行回调，但有一点要注意，**此回调是异步执行的！**所以我们如果想要知道发送后到底是成功还是失败，需要进行等待



使用CountDownLatch，在调用前设置CountDownLatch的值为1，将CountDownLatch的countDown操作放在回调函数中，当向Kafka发送消息完毕后，会调用回调方法，回调方法执行完成后解除门栓，主进程就能继续向下执行了：根据回调函数结果判断消息是否投递成功，成功的话返回前端订单编号且前端开始轮询；失败的话，直接抛出异常，告知用户下单失败。



在向Kafka发送消息时，采用CompletableFuture异步获取发送结果

```java
    public void sendMessage(String message, SuccessCallback<SendResult<String, String>> successCallback,
                            FailureCallback failureCallback) {
        log.info("创建订单kafka发送消息 消息体 : {}", message);
        CompletableFuture<SendResult<String, String>> completableFuture =
                kafkaTemplate.send(SpringUtil.getPrefixDistinctionName() + "-" + kafkaTopic.getTopic(), message);
        completableFuture.whenComplete((result, ex) -> {
            if(Objects.isNull(ex)){
                successCallback.onSuccess(result);
            } else{
                failureCallback.onFailure(ex);
            }
        });
    }
```



**消费消息队列的订单消息**

如果消费到消息时，延迟时间超过了5s，那么此订单丢弃，调用取消订单的函数，将库存中的余票数量和座位状态回滚回去；反之，调用订单服务创建订单并写入数据库和缓存中。



**为什么要将订单放入redis中**

除了添加到数据库中，还将订单编号放到了redis中，这是为什么？

其实是为了前端服务在轮训时，能更快的查询到。因为redis的性能要比数据库强太多，所以让前端服务直接去redis查询就可以了，数据也不用复杂，订单编号就可以。但是不能一直存放，要设置一个过期时间，这里设置了1分钟



**如果出现丢订单问题怎么办**

严格来说，这样做确实是不如同步创建订单保险，但我们要考虑实际应用的场景，能使用这种方式的一定是生成订单的并发非常高的，这时的痛点是要怎么解决高并发的tps问题，想办法提高吞吐量。至于丢订单问题，就算真的丢了又怎么样呢，订单并发量这么高，丢少量的单其实没有什么问题，依旧能保证全部售卖完毕





# 订单服务

## 订单生成失败后如何快速回滚数据

**接口：com.damai.service.ProgramOrderService#doCreate**

```java
{   
    ......
	//调用订单服务
    String orderNumber;
    ApiResponse<String> createOrderResponse = orderClient.create(orderCreateDto);
    if (Objects.equals(createOrderResponse.getCode(), BaseCode.SUCCESS.getCode())) {
        orderNumber = createOrderResponse.getData();
    }else {
        //订单创建失败将操作缓存中的数据还原
        updateProgramCacheDataResolution(programId,purchaseSeatList,OrderStatus.CANCEL);
        log.error("创建订单失败 需人工处理 orderCreateDto : {}",JSON.toJSONString(orderCreateDto));
        throw new DaMaiFrameException(createOrderResponse);
    }
    ......
}
```

当调用订单服务创建订单失败后，需要执行`updateProgramCacheDataResolution` 方法将数据回滚回去，这个方法负责处理生成订单的扣减数据和取消订单的回滚数据两种操作。



## 取消订单和延迟关闭后如何正确处理数据

![订单延迟关闭执行流程.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1711550596096-e63afbf5-2fb8-42e1-85c9-2c6c40389930.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_26%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_679%2Climit_0)

**updateProgramRelatedDataResolution** 方法同样也是负责两种状态的操作，订单取消和订单支付，这两种操作刚好也都是相反的

- 订单取消是要恢复余票数量，将座位状态从锁定中修改为未售卖
- 订单支付是将座位状态从锁定中修改为已售卖



这里只分析取消订单的流程：

- 查询主订单 如果为空，抛出异常。如果状态为已取消、已支付、已退单的状态，则不再执行
- 将主订单修改为已取消状态
- 将购票人订单修改为已取消状态
- 查询该订单下的购票人订单列表，过滤出购票人的座位列表
- 如果是取消操作，那么把用户下该节目的订单数量要-1
- 将节目id、座位列表、订单取消类型传入lua执行处理器
- 组装lua执行需要的数据



## 订单超时取消功能为什么使用Redisson而不是用RocketMQ

**RocketMQ的缺点**

- RocketMQ的延迟消息依赖固定的时间级别，例如 **1s、5s、10s、30s...**，但不支持自定义毫秒级的精确延迟。

- MQ **是基于 FIFO（先进先出）模式**，如果MQ发生消息堆积，RocketMQ的延迟消息可能会之后，导致任务无法按预期时间执行。

**Redisson的解决方案**

- Redisson **基于 Redis 的 `ZSet`（有序集合）** 进行延迟任务调度，任务按照 `score` 排序，Redisson 只会获取 **到期的任务**，不受未到期任务数量影响，**即使有 1000 万条任务堆积，Redisson 也只会取出到期任务进行消费**，而不会遍历所有任务。**可以做到毫秒级精度**，保证任务按时执行。
- **不会因消息堆积而影响执行时间**，即使 Redis 任务积压，也能通过**定时轮询** 精准执行任务。
- 当 Redisson 客户端重启时，`RDelayedQueue` 的状态会被自动恢复，因为其状态是持久化在 Redis 中的。这意味着即使应用重启，延迟队列的功能也不会受到影响



且能减少一个中间件依赖，因为几乎所有项目都会依赖redis



## 生成订单后的支付流程

![支付流程.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723691350784-6d8cc0ea-fbe9-4ed7-b221-441fdf9988e4.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_16%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

**接口：com.damai.controller.OrderController#pay**

```java
@ApiOperation(value = "订单支付")
@PostMapping(value = "/pay")
public ApiResponse<String> pay(@Valid @RequestBody OrderPayDto orderPayDto) {
    return ApiResponse.ok(orderService.pay(orderPayDto));
}
```

订单服务的支付方法流程并不复杂，验证订单状态**必须是未支付的状态才可以**，其余状态都是异常状态。如果是未支付状态，那么就调用支付服务进行支付，所以在支付流程中，**支付服务的执行才是重头戏**





# 支付服务

## 如何进行订单支付

**接口：com.damai.controller.PayController#commonPay**

首先，对支付方法加分布式锁防止多次支付成功，不依赖第三方支付的幂等性。

在流程中，首先就是通过传入的订单编号查询账单是否存在，如果这是第一次正在支付流程，正常来说账单肯定是不存在的，如果存在了，说明之前支付过，直接抛出异常

接着开始调用支付宝或者微信的处理器来进行支付，这里使用了策略模式，来将支付宝/微信抽取成两种不同的策略，如果后续有银联、翼支付等多种支付渠道的话，直接添加策略即可





# 其它问题

## 项目中遇到的最大的问题

查询节目详情需要查很多表，比如节目演出时间、节目数据、节目分组、用户节目订单数量、节目类型、节目票档等，是同步的尽管可以用redis和本地缓存查询，但可以使用CompletableFuture进行优化。



**ComplatableFuture优化出现的bug**





**分布式链路id的传递出现的bug**

当通过线程池新建一个子线程去执行任务时，通过日志发现，MDC中的链路id并没有传递到子线程中，通过debug发现是因为父线程与子线程的MDC并不共享。







## redis宕机如何解决？如果是项目上线的宕机呢

如果只有一台redis，肯定会造成数据丢失



多台redis或者是redis集群

- 从的redis宕机
  - 主从复制，从节点会从主节点redis中读取主节点的操作日志，来达到主从复制
    1. 如果未开启了数据持久化，只要把从节点redis重新启动，再和主节点进行连接进行全量备份即可
    2. 如果开启了数据持久化，可以直接连接到主节点，实现增量备份
- 主的redis宕机
  - 若没有持久化，重新启动主节点会造成数据丢失
    1. 先把从节点升级为主redis，执行slave of one命令
    2. 原来的主节点可以重新启动，作为新的主节点的从节点，进行主从复制

可以使用redis提供的哨兵机制来简化上面的操作



## 项目中用到了哪些设计模式

**装饰器模式**

**责任链模式**

**模板模式**





## 如何解决并发量大时Kafka出现消息堆积的问题



## 项目怎么实现防刷



## 如何确保多级缓存的一致性

对于这种一致性问题，可以使用通用的方案，也就是当修改数据库中的数据后，将对应的缓存清空，Redis的缓存好办，可以直接删除掉

**但是本地缓存就会有个问题，如果存在多实例，那么要怎么处理？**

就拿节目服务来说，假设线上部署了5个实例节点，经过一段时间运行后，每个实例都有了自己的本地缓存，那么如果进行了数据的修改操作后，就要将这5个实例节点的数据都清空

那么要如何通知这5个节点呢？可以有这几种方式：

**1. 定时任务查询**

- 只能应对简单而且数据量小的业务，而且不好估算定时任务的执行时间，频率高了对数据库的压力很大，频率低了缓存又不及时被清除，而且假如某段时间没有修改数据或者主动要失效的操作，那么就白执行了。
- 多实例的情况，就只能每个实例都要查询一遍数据库，属实没有必要

**2. 使用MQ消息中间件**

- 可行
- 但是否有必要在这个轻量级的功能上使用MQ

**3. Redis的PUB/SUB，订阅/发布模式**

- 不支持持久化，即消息只存在于内存，Redis 服务器重启后不会保留历史消息。

**4. Redis的Stream**

- 可以理解成是Redis对消息队列MQ的完善实现，支持分组消费和广播消费，并且可以将消息进行持久化



## Redis怎么实现消息队列

使用Redis实现消息队列的方式有两种，例如：

- **PUB/SUB，订阅/发布模式** 这种发布订阅模式是没有办法进行持久化的，如果出现网络断开、Redis宕机的话，消息就会丢失
- **LPUSH+BRPOP** 或者 **Sorted-Set**的实现，这种可以支持了持久化，但是不支持多播，分组消费的特点



## 项目中是怎么实现雪花算法的

**使用百度开源的UidGenerator**

- 自增列：workerId 基于数据库自增 ID，保证实例唯一

  - 传统 雪花算法（Snowflake） 需要 每个实例唯一的 workerId，但在 分布式环境（多个实例部署） 下，手动分配 `workerId` 容易冲突。

  - 如果使用 机器 IP 计算 workerId，当实例重启、扩容或迁移时，可能会导致 workerId 变化，导致 ID 可能重复。

  - UidGenerator中**workerId 不再手动指定，而是基于数据库的** **`自增 ID`** 进行分配。每个实例 重启时，都会在数据库中插入一条 新的记录，并获得一个全新的 workerId（数据库 `AUTO_INCREMENT` 自增）。

    这样可以确保每个实例的 workerId 唯一，不会发生冲突。

- RingBuffer：预生成 ID，提升性能
  - 使用 RingBuffer 预生成若干个分布式 ID，存储在环形缓冲区中：
    - 当请求 ID 时，直接从 RingBuffer 取值，不需要重新计算，提高性能。
    - 后台线程 持续填充 RingBuffer，保证缓冲区始终有可用 ID。

- 时间递增优化：避免时钟回拨

  - 时间戳不再依赖系统时钟（`System.currentTimeMillis()`），而是用 `AtomicLong` 递增 作为时间基准：

    ```java
    private AtomicLong time = new AtomicLong(System.currentTimeMillis());
    
    public long nextTimestamp() {
        return time.incrementAndGet(); // 使用自增时间戳
    }
    ```

    

## 假如用户支付了，因为网络或支付宝内部原因，支付宝没有回调，而且主动查询账单不存在，该怎么办

这种支付系统要设计账单表，存的就是支付宝或者微信的中的流水号，等有账单不存在的话，那这个流水号去支付宝或者微信平台上去查。项目中有设计这个账单功能。



## 一个商家的余额，在一秒内，1000个不同用户请求去增减，超过了数据库的承受能力怎么办（限流是基本的不用答）

最简单就是分布式锁，不会超过数据库极限了。如果想提高并发，那就把余额放到redis中，然后在redis中验证扣减余额，同步的话，就放在mq里，再有个服务消费mq，然后去扣减数据库。大麦中的订单v4不就是这么干的吗



## 有一个大表，要把它拆成100个小表，怎么拆？保证业务一直运行

在应用层或通过中间件，实现对新增和更新操作同时写入原大表和对应的小表（双写），保证新数据在两个存储中都一致。对历史数据，采用增量同步方案，利用工具（如Canal、DataX或自研脚本）将大表数据逐条或批量同步到目标小表中。在同步过程中，通过对比数据校验新旧表数据一致性，确保拆分过程中数据不丢失或错乱。



## 项目中还有什么可以进一步优化的点

比如将购票数量在redis可以进一步拆分 实现同一个票档也能分片存储。

```
SET ticket_vip_123_0 = 200  # 票档 ID 123，第 1 片，200 张票
SET ticket_vip_123_1 = 200  # 票档 ID 123，第 2 片，200 张票
SET ticket_vip_123_2 = 200  # 票档 ID 123，第 3 片，200 张票
SET ticket_vip_123_3 = 200  # 票档 ID 123，第 4 片，200 张票
SET ticket_vip_123_4 = 200  # 票档 ID 123，第 5 片，200 张票
```

多个用户请求时，随机命中一个 Key，降低单点压力。



## APP崩溃或不响应转圈等待的情况问题主要出现在哪

接口执行时间长一直不响应会有这个问题，避免此问题的方法是做法接口的各种超时，比如数据库超时，rpc调用超时，调用第三方超时，设置好超时时间。





# 简历上的内容（需要重点掌握）

## TransmittableThreadLocal原理

增强 `InheritableThreadLocal`

- `ThreadLocal` 默认不会在线程池中传递。
- `InheritableThreadLocal` 仅适用于 `new Thread()` 创建的新线程，但线程池复用时依然丢失。
- `TransmittableThreadLocal` 解决了线程池复用导致的上下文丢失问题。





## Redisson的分布式锁原理

Redisson是基于Redis的分布式锁实现，提供可重入锁、公平锁、读写锁等多种分布式锁机制。其核心原理**基于Redis的Hash **来实现互斥锁，同时利用Lua脚本保证原子性，并结合**Watchdog机制**防止锁超时被误释放。



**加锁过程**

- 尝试加锁

  - 使用Lua脚本进行加锁保证原子性
  - 首先使用redis的`exists`命令判断锁是否存在，如果不存在执行加锁，如果存在，判断此线程id在map中是否存在，存在就给value加1（重入锁）
  - 使用redis的`hset`命令加锁，key是锁的名字，map中的key是线程id，value就是锁的重入次数。
  - 设置此锁的过期时间是30s

  ![image-20250217232350994](C:/Users/shiyu/AppData/Roaming/Typora/typora-user-images/image-20250217232350994.png)

- 锁记录线程ID
  - `key` 值存储 **线程 ID（UUID + 线程标识）**
  - 用于 **可重入锁** 识别同一线程

- 失败则自旋等待
  - 如果锁已存在，**线程阻塞并监听锁释放通知**



**Watchdog机制**

默认锁超时时间：30 s

Watchdog 机制：

- 每 10 秒自动续期
- 如果这个锁的map的key中存在线程id，就会对其进行续期，过期时间为30s（只要当前实例没有挂掉并且没有主动释放锁，看门狗就会对锁进行主动续约）



**解锁过程**

- 检查锁持有者

  - 只有锁持有者才能释放锁

- 递减锁计数器

  - 如果计数器 `count > 1`，只减少计数，不删除 `key`

  - 否则，删除 `key` 并通知 **阻塞队列** 继续尝试加锁

  ```java
  if redis.call('GET', KEYS[1]) == ARGV[1] then
      if redis.call('DECR', KEYS[1]) == 0 then
          redis.call('DEL', KEYS[1])
          redis.call('PUBLISH', KEYS[2], ARGV[1])
      end
      return 1
  else
      return 0
  end
  ```

  

## Redisson延迟队列的原理

- 客户端启动，redisson先订阅一个key，同时 BLPOP key 0 无限监听一个阻塞队列（等里面有数据了就返回）。
- 当有数据put时，redisson先把数据放到一个zset集合（按延时到期时间的时间戳为分数排序），同时发布上面订阅的key，发布内容为数据到期的timeout，此时客户端进程开启一个延时任务，延时时间为发布的timeout。
- 客户端进程的延时任务到了时间执行，从zset分页取出过了当前时间的数据，然后将数据rpush到第一步的阻塞队列里。然后将当前数据从zset移除，取完之后，又执行 BLPOP key 0 无限监听一个阻塞队列。
- 上一步客户端监听的阻塞队列返回取到数据，回调到 RBlockingQueue 的 take方法。于是，我们就收到了数据。

https://zhuanlan.zhihu.com/p/343811173



## 幂等怎么实现的



## Kafka的实现原理





## 如何自定义分布式链路id

**为什么需要分布式链路id？**

- 跟踪请求流程：在一个由多个微服务组成的系统中，一个外部请求可能需要通过多个服务才能完成。分布式链路ID允许我们将这个请求经过的所有服务连接起来，形成一个完整的链路图，从而能够追踪请求的整个流程
- 性能监控：通过分析请求链路中各个环节的处理时间，可以识别出系统的性能瓶颈，为性能优化提供有力的数据支持
- 故障定位：在发生错误或异常时，分布式链路ID可以帮助快速定位问题发生的服务和位置，加速故障排除和修复过程



**为什么不用skywalking**

SKywalking的性能消耗其实并不低，它的原理是**使用字节码增强生成代理类，然后在本地内存中进行数据的汇总，接着使用Grpc的传输协议到控制台中。**

就这么一套下来，对cpu和内存其实都是有压力的，另外而言其实并不是每个链路都要图形化的显示。**但每个链路调用确实都需要链路id来串联起来。**

所以我们自己来设计出链路id的功能，这样不会怎么影响性能，也能实现这个核心功能



**实现思路**

首先这个链路id是公共参数，不能影响主业务，所以一般在网关层生成后会将id放入Request请求头中传递下去。

接着在每个业务服务会执行一个过滤器，此过滤器从Request头部取出id放入日志配置Slf4j的MDC作用域中，然后在Logback/Log4j2的配置中配置id的输出，如下

```xml
<!--输出控制台的配置-->
<Console name="Console" target="SYSTEM_OUT">
    <!-- 输出日志的格式 -->
    <PatternLayout pattern="[test-service] [%X{traceId}] %d{yyyy-MM-dd HH:mm:ss} %5p %c{1}:%L - %m%n"/>
</Console>
```

在实际的项目中，这个id是由nginx来生成的，如果请求没有经过nginx，那么gateway来生成然后传递到下一个业务服务中，然后再依次的传递下去，**然而在传递过程中会发生各种各样的问题**。

实现思路其实和`TransmittableThreadLocal`差不多，但要精简许多，就是**每次在线程执行前，先将主线程中的数据传递到子线程中，子线程再获取一个副本，当子线程任务执行完后，再将刚才的副本设置回去。**



**流程图**

![0ce0c15f-209e-4322-b07a-1c6fbb49c552](C:/Users/shiyu/Desktop/0ce0c15f-209e-4322-b07a-1c6fbb49c552.png)



**存在的问题**

- 使用Feign的问题

  微服务之间的调用常见使用Feign，比如A调用B服务，默认Feign并不会将A服务请求头中的id自动的传递给B服务的请求头中，所以需要我们额外配置。

  ```java
  public class FeignRequestInterceptor implements RequestInterceptor {
      
      @Override
      public void apply(final RequestTemplate template) {
          try {
              RequestAttributes ra = RequestContextHolder.getRequestAttributes();
              if (ra != null) {
                  ServletRequestAttributes sra = (ServletRequestAttributes) ra;
                  HttpServletRequest request = sra.getRequest();
                  String traceId = request.getHeader(TRACE_ID);
                  String code = request.getHeader(CODE);
                  //将traceId传递到下一个服务中
                  template.header(TRACE_ID,traceId);
                  //将code传递到下一个服务中
                  template.header(CODE,code);
              }
          }catch (Exception e) {
              log.error("FeignRequestInterceptor apply error",e);
          }
      }
  ```

  `RequestInterceptor` 是 Feign 提供的接口，允许在每次发起 Feign 请求时执行拦截逻辑。

  `apply(RequestTemplate template)` 方法会在Feign 发起请求前执行，可以修改请求头、参数等信息。

  

- 使用线程池的问题

  Request的作用域是个ThreadLocal，日志中的MDC本质也是ThreadLocal，又或者有其他的数据需要放到ThreadLocal中，而ThreadLocal和线程是绑定的，这就导致了在线程池中是获取不到ThreadLocal中的数据的，ThreadLocal可以做到线程隔离原理是在每个线程存在一个Map，key是ThreadLocal对象本身，value是值。但在线程池情况下就无法传递参数了



**具体实现**

- 额外定制线程池

  而通常链路id是放在Request的请求头中进行存储的的，而**Request的作用域其实就是个ThreadLocal**，还有就是日志中的MDC本质其实也是个**ThreadLocal**，又或者有其他的数据需要放到ThreadLocal中，而ThreadLocal和线程是绑定的，这就导致了在线程池中是获取不到ThreadLocal中的数据的，这就需要我们将设计出的线程池要解决这个问题。

- BaseThreadPool对线程池的增强

  ![线程池执行流程图.png](https://cdn.nlark.com/yuque/0/2024/png/22643320/1723693164808-7aef62e2-64fa-4fce-ab99-4de1cd40180c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_38%2Ctext_6Zi_5pif5LiN5piv56iL5bqP5ZGY%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)	具体的实现是使用装饰器模式，增强Runnable和Callable接口，在run()之前设置当前线程的ThreadLocal和MDC为父线程的ThreadLocal和MDC，并将当前线程的ThreadLocal和MDC保存下来为了在run()之后再还原回去。

  ```java
      private static Map<String,Map<String,String>> preprocess(final Map<String, String> parentMdcContext, final Map<String, String> parentHoldContext){
          Map<String,Map<String,String>> map = new HashMap<>();
          //获取本线程的hold上下文
          Map<String, String> holdContext = BaseParameterHolder.getParameterMap();
          //获取本线程的MDC上下文
          Map<String, String> mdcContext = MDC.getCopyOfContextMap();
          //如果父线程的MDC上下文为空，则清空子线程的
          if (parentMdcContext == null) {
              MDC.clear();
          } else {
              //否则将父线程的设置到这次本线程中
              MDC.setContextMap(parentMdcContext);
          }
          //如果父线程的hold上下文为空，则清空子线程的
          if (parentHoldContext == null) {
              BaseParameterHolder.removeParameterMap();
          } else {
              //否则将父线程的设置到这次本线程中
              BaseParameterHolder.setParameterMap(parentHoldContext);
          }
          map.put("holdContext",holdContext);
          map.put("mdcContext",mdcContext);
          return map;
      }
  ```

  ```java
      private static void postProcess(Map<String, String> mdcContext, Map<String, String> holdContext){
          //如果本线程MDC上下文为空，直接清除掉
          if (mdcContext == null) {
              MDC.clear();
          } else {
              //否则，将本线程的上下文恢复回去
              MDC.setContextMap(mdcContext);
          }
          //如果本线程hold上下文为空，直接清除掉
          if (holdContext == null) {
              BaseParameterHolder.removeParameterMap();
          } else {
              //否则，将本线程的上下文恢复回去
              BaseParameterHolder.setParameterMap(holdContext);
          }
      }
  }
  ```

  

## 项目中用到了哪些设计模式

**责任链模式**

- 各种验证流程



**装饰器模式**

- 通过额外定制线程池实现MDC和ThreadLocal的传递



**模板方法模式**

- 初始化组件

  - 负责执行的公共抽象层，定义了execute()方法，此方法内调用的type方法交给其子类去实现。**execute()就是模板方法**

    - 从spring中获取InitializeHandler类型的bean集合
    - 将集合的type方法和具体实现类的type()方法进行匹配
    - 将集合中匹配到的元素通过executeOrder()方法进行排序
    - 将排序后的集合进行循环执行executeInit方法

    

    这里留出了`type()`方法用于给不同实现类来实现，从而识别出各自的初始化类型

  - `ApplicationCommandLineRunnerExecute`就是执行`CommandLineRunner`类型
  - `ApplicationInitializingBeanExecute`就是执行`InitializingBean`类型
  - `ApplicationPostConstructExecute`就是执行`@PostConstruct`类型
  - `ApplicationStartEventListenerExecute`就是执行`ApplicationListener*<ApplicationStartedEvent>`类型



## 现在分库分表是基于ID取余，那如果现在需要对库表进行扩容，那取余的方式会不会限制后续的拓展

如果只是取余的方式会限制，因为后续扩容需要重新计算映射



## 如果取余不均匀，有很多数据落到了同一张表上，后续要怎么处理

- 取余前添加哈希操作，对哈希值进行取模，打散数据分布

- 添加虚拟分片，再从虚拟分片映射到实际分片上
- 可以加一个统计不同业务在不同表中的数量，不要求非常准确，大概一个数就可以，再每次进行分片算法时，根据统计的数量判断哪个表数量少，然后直接分片到那张



