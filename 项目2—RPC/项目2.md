## 常见的限流算法有哪些

**1. 计数器（固定窗口）**

**2. 滑动窗口**

**3. 漏桶**

**4. 令牌桶**



## 为什么需要熔断

![img](https://cdn.nlark.com/yuque/0/2024/png/45263982/1731565276196-b21b68e2-05a4-440f-92fe-5f4afed29e1a.png?x-oss-process=image%2Fformat%2Cwebp)

在整个调用链中，只要中间有一个服务出现问题，都可能会引起上游的所有服务出现一系列的问题，甚至会引起整个调用链的服务都宕机。

因此，在一个服务作为调用端调用另外一个服务时，为了防止被调用的服务出现问题而影响到作为调用端的这个服务，这个服务也需要进行自我保护。而最有效的自我保护方式就是熔断。





实现的rpc跟dubbo有什么区别，为什么不用dubbo要用这个rpc？ 如果把rpc改成分布式架构，应该怎么改 ，哪一部分要改成分本式？注册中心改成分布式，客户端的接口的幂等性和多个分布式注册中心之间的数据一致性怎么保证或者平衡？心跳检测是在一个单线程定时器里，这一部分要不要分布式一下？ 服务端节点应该怎么优雅地上下线？客户端什么时候会拉取最新的服务节点？拉取的是所有的服务节点还是部分服务节点？为什么？ 客户端和服务端的定时器的区别？ rpc为什么要基于tcp？能不能用udp？该怎么改或者定怎样的传输协议，可靠性传输相关的要不要考虑一下？ 能不能在除了rpc的应用层，在别的层（比如传输层和网络层）做负载均衡？大概该怎么做？



## 为什么选择Netty

**1. 高性能NIO**

Netty基于NIO，采用Reactor模型，相比传统的BIO，能在高并发环境下提供更高的吞吐量和更低的资源消耗。

- 零拷贝：减少数据在用户态和内核态之间的拷贝，提高性能
- IO多路复用：通过Selector监听多个连接时间，避免了BIO一个连接就要建立一个线程的连接开销
- 池化的ByteBuf：Netty提供了更高效的内存管理，减少GC负担



**2. 事件驱动架构**

Netty提供了基于Pipeline + ChannelHandler的事件处理机制

- 解耦I/O处理与业务逻辑，不同Handler负责不同功能（编码/解码、超时控制、业务处理）
- 异步非阻塞，基于Future和Promise进行回调，提高吞吐量



## 介绍一下netty底层具体是怎么实现的？

Netty底层实现是基于主从Reactor模型的

![img](https://dongzl.github.io/netty-handbook/_media/chapter05/chapter05_08.png)

1. `BossGroup` 线程维护 `Selector`，只关注 `Accecpt`
2. 当接收到 `Accept` 事件，获取到对应的 `SocketChannel`，封装成 `NIOScoketChannel` 并注册到 `Worker` 线程（事件循环），并进行维护
3. 当 `Worker` 线程监听到 `Selector` 中通道发生自己感兴趣的事件后，就进行处理（就由 `handler`），注意 `handler` 已经加入到通道



![img](https://dongzl.github.io/netty-handbook/_media/chapter05/chapter05_10.png)

1. `Netty` 抽象出两组线程池 `BossGroup` 专门负责接收客户端的连接，`WorkerGroup` 专门负责网络的读写
2. `BossGroup` 和 `WorkerGroup` 类型都是 `NioEventLoopGroup`
3. `NioEventLoopGroup` 相当于一个事件循环组，这个组中含有多个事件循环，每一个事件循环是 `NioEventLoop`
4. `NioEventLoop` 表示一个不断循环的执行处理任务的线程，每个 `NioEventLoop` 都有一个 `Selector`，用于监听绑定在其上的 `socket` 的网络通讯
5. `NioEventLoopGroup` 可以有多个线程，即可以含有多个 `NioEventLoop`
6. 每个`BossNioEventLoop`循环执行的步骤有3步
   - 轮询 `accept` 事件
   - 处理 `accept` 事件，与 `client` 建立连接，生成 `NioScocketChannel`，并将其注册到某个 `worker` `NIOEventLoop` 上的 `Selector`
   - 处理任务队列的任务，即 `runAllTasks`
7. 每个`Worker NIOEventLoop`循环执行的步骤
   - 轮询 `read`，`write` 事件
   - 处理 `I/O` 事件，即 `read`，`write` 事件，在对应 `NioScocketChannel` 处理
   - 处理任务队列的任务，即 `runAllTasks`
8. 每个 `Worker` `NIOEventLoop` 处理业务时，会使用 `pipeline`（管道），`pipeline` 中包含了 `channel`，即通过 `pipeline` 可以获取到对应通道，管道中维护了很多的处理器



## Netty用到了多路复用，但是多路复用有好几种方式，netty用的是哪一种？怎么用的？

Linux使用epoll

Windows使用select



## Netty底层用到了EventLoop，selector，怎么用的？

`EventLoop` 是 Netty 的 事件循环（线程），负责 监听 I/O 事件、执行任务队列、定时任务。

`EventLoopGroup` 是 `EventLoop` 的 线程池，每个 `EventLoop` 绑定 一个 Selector，用于监听多个 `Channel`。



## TCP粘包是什么？怎么解决？

- TCP 是流式协议，没有消息边界，数据可能被 拆分 或 合并 发送。

- 操作系统的 TCP 发送缓冲区优化：多个小数据包可能被合并发送，以减少网络资源消耗。

- 接收端 `read()` 读取不完整：`read()` 可能读取一个完整消息，也可能读到多个消息（粘包），或者只读到一部分（拆包）。



## 在设计好私有协议的基础上，应该如何优化进一步提升高并发性能以及消息传输效率？

- 消息压缩
- 序列化优化：使用高效的二进制序列化



